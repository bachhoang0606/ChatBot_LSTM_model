{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chatbot.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdKHYQ1L8y92","executionInfo":{"status":"ok","timestamp":1655192390425,"user_tz":-420,"elapsed":2733,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}},"outputId":"364a771d-c5e9-4b33-e66b-8cb9478585f6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Colab Notebooks/Chatbot/WikiQACorpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7wfy71RdXJ1","executionInfo":{"status":"ok","timestamp":1655192390426,"user_tz":-420,"elapsed":5,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}},"outputId":"208cfdd6-eb8c-44c8-f913-4bede76a61c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Chatbot/WikiQACorpus\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"IsWnxQ2B7OOa","executionInfo":{"status":"ok","timestamp":1655192395884,"user_tz":-420,"elapsed":5461,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}},"outputId":"c7bd9e56-a007-43d2-af1b-c3e5291d7930"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'how are glacier caves formed ?\\tA partly submerged glacier cave on Perito Moreno Glacier .\\t0\\nhow are glacier caves formed ?\\tThe ice facade is approximately 60 m high\\t0\\nhow are glacier caves formed ?\\tIc'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["from os import read\n","from keras.layers import Embedding, LSTM, Dropout,Dense\n","from keras import Input\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","def doc_file(filename):\n","  f = open(filename, 'r')\n","  text = f.read()\n","  f.close()\n","  return text\n","\n","train_doc = doc_file('WikiQA-train.txt')\n","test_doc = doc_file('WikiQA-test.txt')\n","dev_doc = doc_file('WikiQA-dev.txt')\n","\n","train_doc[:200]"]},{"cell_type":"code","source":["import string\n","\n","# preprocessing text\n","def clean_data(text):\n","  # prepare translation table for removing punctuation\n","  table = str.maketrans('', '', string.punctuation)\n","  desc = text\n","  # convert to lower case\n","  desc = desc.lower()\n","  # tokenize\n","  desc = desc.split()\n","  # remove punctualtion fro each token\n","  desc = [w.translate(table) for w in desc]\n","  # remode hanging 's' and 'a'\n","  desc = [word for word in desc if len(word) > 1]\n","  # remove tokens withs numbers in them\n","  desc = [word for word in desc if word.isalpha()]\n","  return ' '.join(desc)"],"metadata":{"id":"h_Aayg-C-qJ5","executionInfo":{"status":"ok","timestamp":1655192395886,"user_tz":-420,"elapsed":9,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def split_q_and_a(doc):\n","  doc_dict = dict()\n","  # split doc to question and answser in one line\n","  for QA in doc.split('\\n'):\n","    # pass if not have anything\n","    if len(QA) < 1:\n","      continue\n","    # split QA to question and answser\n","    var = QA.split('\\t')\n","    question = clean_data(var[0])\n","    answser = clean_data(var[1])\n","    # if one question and answser is list() \n","    if question not in doc_dict:\n","      doc_dict[question] = list()\n","    doc_dict[question].append('startseq ' + answser  + ' endseq')\n","  \n","  return doc_dict\n","    \n","train_dict = split_q_and_a(train_doc)\n","test_dict = split_q_and_a(test_doc)\n","dev_dict = split_q_and_a(dev_doc)"],"metadata":{"id":"aAOVuCA49m3s","executionInfo":{"status":"ok","timestamp":1655192396498,"user_tz":-420,"elapsed":619,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","vocabary_size = 2000\n","tokenizer = Tokenizer(num_words=vocabary_size, oov_token=\"<OOV>\")\n","\n","def data_tokenizer(data_dict):\n","  data = list()\n","  for key, val in data_dict.items():\n","    data.append(key)\n","    for i in range(len(val)):\n","      data.append(val[i])\n","  return data\n","\n","tokenizer.fit_on_texts(data_tokenizer(train_dict))"],"metadata":{"id":"6FjsRVYUCpQv","executionInfo":{"status":"ok","timestamp":1655192397890,"user_tz":-420,"elapsed":1394,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["haha =  tokenizer.texts_to_sequences(['hoang xuan back'])\n","pad_sequences(haha, maxlen=100)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgxj6qNK5V-x","executionInfo":{"status":"ok","timestamp":1655192397891,"user_tz":-420,"elapsed":10,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}},"outputId":"9f2c0d92-9980-4955-d260-95564b9d3074"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   0,   0,   1,   1, 429], dtype=int32)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","def data_to_sequence(data_dict, maxlen, batch_size):\n","  num = 0\n","  X1, X2, y  = list(), list(), list()\n","  while 1:\n","    for question, answser in data_dict.items():\n","        for i in range(len(answser)):\n","          x2y = answser[i].split()\n","          for j in range(1, len(x2y)):\n","            # input sequence of question\n","            x1 = tokenizer.texts_to_sequences([question])\n","            X1.append(pad_sequences(x1, maxlen=maxlen)[0])\n","            # input sequence of answser\n","            x2 = tokenizer.texts_to_sequences([' '.join(x2y[:j])])\n","            X2.append(pad_sequences(x2, maxlen=maxlen)[0])\n","            # ouput\n","            out = tokenizer.texts_to_sequences(x2y[j])\n","            y.append(to_categorical(out,num_classes=20000)[0])\n","            num+=1\n","          if num==batch_size:\n","            yield X1, X2, y\n","            X1, X2, y  = list(), list(), list()\n","            num = 0"],"metadata":{"id":"oc-dG9ibnziu","executionInfo":{"status":"ok","timestamp":1655192397891,"user_tz":-420,"elapsed":9,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["X1, X2, y = data_to_sequence(train_dict, 100, 6)"],"metadata":{"id":"zAaCAq_gKvym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X1[5]"],"metadata":{"id":"_hsQLO129Nhf","executionInfo":{"status":"aborted","timestamp":1655192397892,"user_tz":-420,"elapsed":7,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X2[5]"],"metadata":{"id":"0hVhODJ19O_2","executionInfo":{"status":"aborted","timestamp":1655192397892,"user_tz":-420,"elapsed":7,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y[5]"],"metadata":{"id":"_9eyVA-M9QGY","executionInfo":{"status":"aborted","timestamp":1655192397892,"user_tz":-420,"elapsed":7,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.word_index[]"],"metadata":{"id":"p_2CddH39TYG","executionInfo":{"status":"aborted","timestamp":1655192397893,"user_tz":-420,"elapsed":8,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.backend import dropout\n","from keras.layers.merge import add\n","from keras.models import Model\n","# model\n","input1 = Input(shape=(maxlen, ))\n","qe1 = Embedding(vocabary_size, 512)\n","qe2 = Dropout(0.5)(qe1)\n","qe3 = LSTM(256)(qe2)\n","input2 = Input(shape=(maxlen, ))\n","ae1 = Embedding(vocabary_size, 512)\n","ae2 = Dropout(0.5)(qe1)\n","ae3 = LSTM(256)(qe2)\n","\n","decoder1 = add([ae3, qe3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocabary_size, activation='softmax')(decoder2)\n","model = Model(inputs=[input1, input2], outputs=outputs)\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.summary()"],"metadata":{"id":"8-SUVuvltTg1","executionInfo":{"status":"aborted","timestamp":1655192397893,"user_tz":-420,"elapsed":8,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["maxlen=100\n","epochs = 10\n","batch_size = 10\n","steps = len(train_dict)//batch_size\n","for i in range(epochs):\n","  X1, X2, y = data_to_sequence(train_dict, maxlen, batch_size)\n","  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)"],"metadata":{"id":"RbjxKLv6pNhu","colab":{"base_uri":"https://localhost:8080/","height":188},"executionInfo":{"status":"error","timestamp":1655192397894,"user_tz":-420,"elapsed":11,"user":{"displayName":"Bách Hoàng","userId":"04452259599804220022"}},"outputId":"8da8f66c-a1c6-4782-f77d-f7357a5d97bd"},"execution_count":9,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-59e7915ca78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: data_to_sequence() missing 1 required positional argument: 'batch_size'"]}]}]}